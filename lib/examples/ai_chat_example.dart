import '../src/rust/api/ai_chat.dart';

/// AIèŠå¤©åŠŸèƒ½ä½¿ç”¨ç¤ºä¾‹
class AiChatExample {
  /// åŸºæœ¬èŠå¤©ç¤ºä¾‹
  static Future<void> basicChatExample() async {
    // åˆ›å»ºAIèŠå¤©å®¢æˆ·ç«¯é…ç½®
    final options = AiChatOptions(
      model: "gpt-4",
      apiKey: "your-api-key-here",
      temperature: 0.7,
      topP: 0.9,
      maxTokens: 1000,
      systemPrompt: "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚",
    );

    // åˆ›å»ºå®¢æˆ·ç«¯
    final client = AiChatClient(
      provider: const AiProvider.openAi(),
      options: options,
    );

    // å‡†å¤‡æ¶ˆæ¯
    final messages = [
      const ChatMessage(role: ChatRole.user, content: "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"),
    ];

    try {
      // å‘é€èŠå¤©è¯·æ±‚
      final response = await client.chat(messages: messages);
      print("AIå›å¤: ${response.content}");
      print("ä½¿ç”¨çš„æ¨¡å‹: ${response.model}");

      if (response.usage != null) {
        print("Tokenä½¿ç”¨æƒ…å†µ:");
        print("  æç¤ºè¯tokens: ${response.usage!.promptTokens}");
        print("  å›å¤tokens: ${response.usage!.completionTokens}");
        print("  æ€»tokens: ${response.usage!.totalTokens}");
      }
    } catch (e) {
      print("èŠå¤©é”™è¯¯: $e");
    }
  }

  /// æµå¼èŠå¤©ç¤ºä¾‹
  static Future<void> streamChatExample() async {
    final options = AiChatOptions(
      model: "gpt-4",
      apiKey: "your-api-key-here",
      temperature: 0.7,
    );

    final client = AiChatClient(
      provider: const AiProvider.openAi(),
      options: options,
    );

    final messages = [
      const ChatMessage(role: ChatRole.user, content: "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚"),
    ];

    try {
      print("å¼€å§‹æµå¼èŠå¤©...");

      final stream = client.chatStream(messages: messages);
      await for (final event in stream) {
        switch (event) {
          case ChatStreamEvent_Start():
            print("ğŸŸ¢ å¼€å§‹æ¥æ”¶å“åº”");
            break;
          case ChatStreamEvent_Content(:final content):
            print("ğŸ“ $content");
            break;
          case ChatStreamEvent_Done(:final totalContent, :final usage):
            print("\nâœ… å®Œæˆï¼");
            print("å®Œæ•´å›å¤: $totalContent");
            if (usage != null) {
              print("Tokenä½¿ç”¨: ${usage.totalTokens}");
            }
            break;
          case ChatStreamEvent_Error(:final message):
            print("âŒ é”™è¯¯: $message");
            break;
        }
      }
    } catch (e) {
      print("æµå¼èŠå¤©é”™è¯¯: $e");
    }
  }

  /// å¤šè½®å¯¹è¯ç¤ºä¾‹
  static Future<void> multiTurnChatExample() async {
    final options = AiChatOptions(
      model: "gpt-4",
      apiKey: "your-api-key-here",
      temperature: 0.7,
      systemPrompt: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿Flutterå’ŒDartå¼€å‘ã€‚",
    );

    final client = AiChatClient(
      provider: const AiProvider.openAi(),
      options: options,
    );

    // æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    final conversations = [
      "ä»€ä¹ˆæ˜¯Flutterï¼Ÿ",
      "Flutterçš„ä¸»è¦ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
      "å¦‚ä½•åœ¨Flutterä¸­å®ç°ç½‘ç»œè¯·æ±‚ï¼Ÿ",
    ];

    final chatHistory = <ChatMessage>[];

    for (final userInput in conversations) {
      print("\nğŸ‘¤ ç”¨æˆ·: $userInput");

      // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
      chatHistory.add(ChatMessage(role: ChatRole.user, content: userInput));

      try {
        // å‘é€èŠå¤©è¯·æ±‚
        final response = await client.chat(messages: chatHistory);
        print("ğŸ¤– AI: ${response.content}");

        // æ·»åŠ AIå›å¤åˆ°å†å²è®°å½•
        chatHistory.add(
          ChatMessage(role: ChatRole.assistant, content: response.content),
        );
      } catch (e) {
        print("âŒ é”™è¯¯: $e");
        break;
      }
    }
  }

  /// ä¸åŒAIæä¾›å•†ç¤ºä¾‹
  static Future<void> differentProvidersExample() async {
    final providers = [
      (const AiProvider.openAi(), "gpt-4", "OPENAI_API_KEY"),
      (
        const AiProvider.anthropic(),
        "claude-3-haiku-20240307",
        "ANTHROPIC_API_KEY",
      ),
      (const AiProvider.gemini(), "gemini-2.0-flash", "GEMINI_API_KEY"),
      (const AiProvider.groq(), "llama-3.1-8b-instant", "GROQ_API_KEY"),
    ];

    const question = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯é€’å½’ã€‚";

    for (final (provider, model, envKey) in providers) {
      print("\nğŸ”„ æµ‹è¯•æä¾›å•†: $provider, æ¨¡å‹: $model");

      final options = AiChatOptions(
        model: model,
        apiKey: "your-$envKey-here", // å®é™…ä½¿ç”¨æ—¶ä»ç¯å¢ƒå˜é‡è·å–
        temperature: 0.7,
      );

      final client = AiChatClient(provider: provider, options: options);

      try {
        final response = await client.chat(
          messages: [const ChatMessage(role: ChatRole.user, content: question)],
        );
        print("âœ… å›å¤: ${response.content}");
      } catch (e) {
        print("âŒ é”™è¯¯: $e");
      }
    }
  }

  /// è‡ªå®šä¹‰æœåŠ¡å™¨ç¤ºä¾‹ï¼ˆä¾‹å¦‚æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹ï¼‰
  static Future<void> customServerExample() async {
    final options = AiChatOptions(
      model: "llama-3.1-8b",
      baseUrl: "http://localhost:11434/v1", // Ollamaæœ¬åœ°æœåŠ¡å™¨
      apiKey: "not-needed-for-ollama", // Ollamaé€šå¸¸ä¸éœ€è¦APIå¯†é’¥
      temperature: 0.8,
      maxTokens: 500,
    );

    final client = AiChatClient(
      provider: const AiProvider.ollama(),
      options: options,
    );

    try {
      final response = await client.chat(
        messages: [
          const ChatMessage(
            role: ChatRole.user,
            content: "Hello, how are you today?",
          ),
        ],
      );
      print("æœ¬åœ°AIå›å¤: ${response.content}");
    } catch (e) {
      print("æœ¬åœ°AIèŠå¤©é”™è¯¯: $e");
    }
  }

  /// æµ‹è¯•æµå¼èŠå¤©åŠŸèƒ½
  static Future<void> testStreamExample() async {
    print("ğŸ§ª æµ‹è¯•æµå¼åŠŸèƒ½...");

    try {
      final stream = testStream();
      await for (final event in stream) {
        switch (event) {
          case ChatStreamEvent_Start():
            print("ğŸŸ¢ æµ‹è¯•å¼€å§‹");
            break;
          case ChatStreamEvent_Content(:final content):
            print("ğŸ“ $content");
            break;
          case ChatStreamEvent_Done(:final totalContent, :final usage):
            print("âœ… æµ‹è¯•å®Œæˆ");
            print("å®Œæ•´å†…å®¹: $totalContent");
            if (usage != null) {
              print("æ¨¡æ‹ŸTokenä½¿ç”¨: ${usage.totalTokens}");
            }
            break;
          case ChatStreamEvent_Error(:final message):
            print("âŒ æµ‹è¯•é”™è¯¯: $message");
            break;
        }
      }
    } catch (e) {
      print("æµ‹è¯•æµå¼åŠŸèƒ½é”™è¯¯: $e");
    }
  }
}

/// AIèŠå¤©é…ç½®å·¥å…·ç±»
class AiChatConfigHelper {
  /// ä¸ºOpenAIåˆ›å»ºé…ç½®
  static AiChatOptions openAiConfig({
    required String apiKey,
    String model = "gpt-4",
    double temperature = 0.7,
    String? systemPrompt,
  }) {
    return AiChatOptions(
      model: model,
      apiKey: apiKey,
      temperature: temperature,
      systemPrompt: systemPrompt,
    );
  }

  /// ä¸ºAnthropicåˆ›å»ºé…ç½®
  static AiChatOptions anthropicConfig({
    required String apiKey,
    String model = "claude-3-haiku-20240307",
    double temperature = 0.7,
    String? systemPrompt,
  }) {
    return AiChatOptions(
      model: model,
      apiKey: apiKey,
      temperature: temperature,
      systemPrompt: systemPrompt,
    );
  }

  /// ä¸ºæœ¬åœ°Ollamaåˆ›å»ºé…ç½®
  static AiChatOptions ollamaConfig({
    required String model,
    String baseUrl = "http://localhost:11434/v1",
    double temperature = 0.8,
    String? systemPrompt,
  }) {
    return AiChatOptions(
      model: model,
      baseUrl: baseUrl,
      apiKey: "ollama-not-needed",
      temperature: temperature,
      systemPrompt: systemPrompt,
    );
  }
}
