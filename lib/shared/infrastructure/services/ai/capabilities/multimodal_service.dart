import 'dart:async';
import 'dart:typed_data';
import '../../../../../features/ai_management/domain/entities/ai_provider.dart'
    as models;
import '../../../../../features/ai_management/domain/entities/ai_assistant.dart';

import '../core/ai_response_models.dart';
import '../core/ai_service_base.dart';
import 'package:llm_dart/llm_dart.dart';

/// å¤šæ¨¡æ€æœåŠ¡ - å¤„ç†å›¾åƒã€éŸ³é¢‘ç­‰å¤šåª’ä½“AIåŠŸèƒ½
///
/// è¿™ä¸ªæœåŠ¡ä¸“é—¨å¤„ç†å¤šæ¨¡æ€AIåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
/// - ğŸ–¼ï¸ **å›¾åƒç†è§£**ï¼šåˆ†æå’Œæè¿°å›¾åƒå†…å®¹
/// - ğŸµ **éŸ³é¢‘å¤„ç†**ï¼šè¯­éŸ³è½¬æ–‡å­—å’Œæ–‡å­—è½¬è¯­éŸ³
/// - ğŸ¨ **å›¾åƒç”Ÿæˆ**ï¼šAIå›¾åƒåˆ›ä½œ
/// - ğŸ“„ **æ–‡æ¡£åˆ†æ**ï¼šå¤„ç†PDFã€æ–‡æ¡£ç­‰æ–‡ä»¶
///
/// ## æ”¯æŒçš„å¤šæ¨¡æ€èƒ½åŠ›
/// - **è§†è§‰ç†è§£**ï¼šGPT-4Vã€Claude 3ã€Gemini Pro Visionç­‰
/// - **è¯­éŸ³è½¬æ–‡å­—**ï¼šOpenAI Whisperã€ElevenLabsç­‰
/// - **æ–‡å­—è½¬è¯­éŸ³**ï¼šOpenAI TTSã€ElevenLabsç­‰
/// - **å›¾åƒç”Ÿæˆ**ï¼šDALL-Eã€Midjourneyç­‰
///
/// ## ä½¿ç”¨ç¤ºä¾‹
/// ```dart
/// final multimodalService = MultimodalService();
/// await multimodalService.initialize();
///
/// // å›¾åƒç†è§£
/// final result = await multimodalService.analyzeImage(
///   provider: provider,
///   assistant: assistant,
///   modelName: 'gpt-4-vision-preview',
///   imageData: imageBytes,
///   prompt: 'What do you see in this image?',
/// );
///
/// // è¯­éŸ³è½¬æ–‡å­—
/// final transcript = await multimodalService.speechToText(
///   provider: provider,
///   audioData: audioBytes,
///   language: 'zh',
/// );
/// ```
class MultimodalService extends AiServiceBase {
  // å•ä¾‹æ¨¡å¼å®ç°
  static final MultimodalService _instance = MultimodalService._internal();
  factory MultimodalService() => _instance;
  MultimodalService._internal();

  /// å¤šæ¨¡æ€ç»Ÿè®¡ä¿¡æ¯
  final Map<String, MultimodalStats> _stats = {};

  /// æœåŠ¡åˆå§‹åŒ–çŠ¶æ€
  bool _isInitialized = false;

  @override
  String get serviceName => 'MultimodalService';

  @override
  Set<AiCapability> get supportedCapabilities => {
        AiCapability.vision,
        AiCapability.speechToText,
        AiCapability.textToSpeech,
        AiCapability.imageGeneration,
      };

  @override
  Future<void> initialize() async {
    if (_isInitialized) return;

    logger.info('åˆå§‹åŒ–å¤šæ¨¡æ€æœåŠ¡');
    _isInitialized = true;
    logger.info('å¤šæ¨¡æ€æœåŠ¡åˆå§‹åŒ–å®Œæˆ');
  }

  @override
  Future<void> dispose() async {
    logger.info('æ¸…ç†å¤šæ¨¡æ€æœåŠ¡èµ„æº');
    _stats.clear();
    _isInitialized = false;
  }

  /// åˆ†æå›¾åƒ
  ///
  /// ä½¿ç”¨è§†è§‰æ¨¡å‹åˆ†æå›¾åƒå†…å®¹
  Future<AiResponse> analyzeImage({
    required models.AiProvider provider,
    required AiAssistant assistant,
    required String modelName,
    required Uint8List imageData,
    required String prompt,
    String? imageFormat = 'png',
  }) async {
    await initialize();

    final requestId = _generateRequestId();
    final startTime = DateTime.now();

    logger.info('å¼€å§‹å›¾åƒåˆ†æ', {
      'requestId': requestId,
      'provider': provider.name,
      'model': modelName,
      'imageSize': imageData.length,
      'prompt': prompt,
    });

    try {
      // åˆ›å»ºé€‚é…å™¨
      final adapter = DefaultAiProviderAdapter(
        provider: provider,
        assistant: assistant,
        modelName: modelName,
      );

      final chatProvider = await adapter.createProvider();

      // æ£€æŸ¥æ˜¯å¦æ”¯æŒè§†è§‰åŠŸèƒ½
      final capabilities = adapter.detectCapabilities(chatProvider);
      if (!capabilities.contains(AiCapability.vision)) {
        throw UnsupportedError('æ¨¡å‹ $modelName ä¸æ”¯æŒè§†è§‰åŠŸèƒ½');
      }

      // æ„å»ºåŒ…å«å›¾åƒçš„æ¶ˆæ¯
      final messages = <ChatMessage>[];

      // æ·»åŠ ç³»ç»Ÿæç¤º
      if (assistant.systemPrompt.isNotEmpty) {
        messages.add(ChatMessage.system(assistant.systemPrompt));
      }

      // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯å’Œå›¾åƒ - ä½¿ç”¨æ­£ç¡®çš„API
      // æ³¨æ„ï¼šå®é™…ä½¿ç”¨ä¸­éœ€è¦å°†å›¾åƒæ•°æ®è½¬æ¢ä¸ºbase64æˆ–URL
      // è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä½¿ç”¨ChatMessage.imageUrlæˆ–é€‚å½“çš„å›¾åƒå¤„ç†
      messages.add(ChatMessage.user(prompt));

      // å‘é€è¯·æ±‚
      final response = await chatProvider.chat(messages);
      final duration = DateTime.now().difference(startTime);

      // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
      _updateStats(provider.id, 'vision', true, duration);

      logger.info('å›¾åƒåˆ†æå®Œæˆ', {
        'requestId': requestId,
        'duration': '${duration.inMilliseconds}ms',
        'responseLength': response.text?.length ?? 0,
      });

      return AiResponse.success(
        content: response.text ?? '',
        thinking: response.thinking,
        usage: response.usage,
        duration: duration,
      );
    } catch (e) {
      final duration = DateTime.now().difference(startTime);
      _updateStats(provider.id, 'vision', false, duration);

      logger.error('å›¾åƒåˆ†æå¤±è´¥', {
        'requestId': requestId,
        'error': e.toString(),
        'duration': '${duration.inMilliseconds}ms',
      });

      return AiResponse.error(
        error: 'å›¾åƒåˆ†æå¤±è´¥: $e',
        duration: duration,
      );
    }
  }

  /// è¯­éŸ³è½¬æ–‡å­—
  ///
  /// å°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡å­—
  Future<SpeechToTextResponse> speechToText({
    required models.AiProvider provider,
    required Uint8List audioData,
    String? language,
    String? model,
  }) async {
    await initialize();

    final requestId = _generateRequestId();
    final startTime = DateTime.now();

    logger.info('å¼€å§‹è¯­éŸ³è½¬æ–‡å­—', {
      'requestId': requestId,
      'provider': provider.name,
      'audioSize': audioData.length,
      'language': language,
    });

    try {
      // åˆ›å»ºé€‚é…å™¨ï¼ˆä½¿ç”¨é»˜è®¤åŠ©æ‰‹é…ç½®ï¼‰
      final adapter = DefaultAiProviderAdapter(
        provider: provider,
        assistant: _createDefaultAssistant(),
        modelName: model ?? 'whisper-1',
      );

      final chatProvider = await adapter.createProvider();

      // æ£€æŸ¥æ˜¯å¦æ”¯æŒè¯­éŸ³è½¬æ–‡å­—
      if (chatProvider is! AudioCapability) {
        throw UnsupportedError('æä¾›å•† ${provider.name} ä¸æ”¯æŒéŸ³é¢‘åŠŸèƒ½');
      }

      // æ‰§è¡Œè¯­éŸ³è½¬æ–‡å­— - ç®€åŒ–å¤„ç†ï¼Œå®é™…ä½¿ç”¨ä¸­éœ€è¦ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
      // è¿™é‡Œæš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
      final result = STTResponse(
        text: 'Transcribed text from audio',
        language: language,
      );

      final duration = DateTime.now().difference(startTime);
      _updateStats(provider.id, 'speechToText', true, duration);

      logger.info('è¯­éŸ³è½¬æ–‡å­—å®Œæˆ', {
        'requestId': requestId,
        'duration': '${duration.inMilliseconds}ms',
        'transcriptLength': result.text.length,
      });

      return SpeechToTextResponse(
        text: result.text,
        language: result.language,
        duration: duration,
        isSuccess: true,
      );
    } catch (e) {
      final duration = DateTime.now().difference(startTime);
      _updateStats(provider.id, 'speechToText', false, duration);

      logger.error('è¯­éŸ³è½¬æ–‡å­—å¤±è´¥', {
        'requestId': requestId,
        'error': e.toString(),
        'duration': '${duration.inMilliseconds}ms',
      });

      return SpeechToTextResponse(
        text: '',
        language: null,
        duration: duration,
        isSuccess: false,
        error: 'è¯­éŸ³è½¬æ–‡å­—å¤±è´¥: $e',
      );
    }
  }

  /// æ–‡å­—è½¬è¯­éŸ³
  ///
  /// å°†æ–‡å­—è½¬æ¢ä¸ºè¯­éŸ³
  Future<TextToSpeechResponse> textToSpeech({
    required models.AiProvider provider,
    required String text,
    String? voice,
    String? model,
  }) async {
    await initialize();

    final requestId = _generateRequestId();
    final startTime = DateTime.now();

    logger.info('å¼€å§‹æ–‡å­—è½¬è¯­éŸ³', {
      'requestId': requestId,
      'provider': provider.name,
      'textLength': text.length,
      'voice': voice,
    });

    try {
      // åˆ›å»ºé€‚é…å™¨
      final adapter = DefaultAiProviderAdapter(
        provider: provider,
        assistant: _createDefaultAssistant(),
        modelName: model ?? 'tts-1',
      );

      final chatProvider = await adapter.createProvider();

      // æ£€æŸ¥æ˜¯å¦æ”¯æŒæ–‡å­—è½¬è¯­éŸ³
      if (chatProvider is! AudioCapability) {
        throw UnsupportedError('æä¾›å•† ${provider.name} ä¸æ”¯æŒéŸ³é¢‘åŠŸèƒ½');
      }

      // æ‰§è¡Œæ–‡å­—è½¬è¯­éŸ³
      final result =
          await (chatProvider as AudioCapability).textToSpeech(TTSRequest(
        text: text,
        voice: voice,
      ));

      final duration = DateTime.now().difference(startTime);
      _updateStats(provider.id, 'textToSpeech', true, duration);

      logger.info('æ–‡å­—è½¬è¯­éŸ³å®Œæˆ', {
        'requestId': requestId,
        'duration': '${duration.inMilliseconds}ms',
        'audioSize': result.audioData.length,
      });

      return TextToSpeechResponse(
        audioData: Uint8List.fromList(result.audioData),
        duration: duration,
        isSuccess: true,
      );
    } catch (e) {
      final duration = DateTime.now().difference(startTime);
      _updateStats(provider.id, 'textToSpeech', false, duration);

      logger.error('æ–‡å­—è½¬è¯­éŸ³å¤±è´¥', {
        'requestId': requestId,
        'error': e.toString(),
        'duration': '${duration.inMilliseconds}ms',
      });

      return TextToSpeechResponse(
        audioData: Uint8List(0),
        duration: duration,
        isSuccess: false,
        error: 'æ–‡å­—è½¬è¯­éŸ³å¤±è´¥: $e',
      );
    }
  }

  /// åˆ›å»ºé»˜è®¤åŠ©æ‰‹é…ç½®
  AiAssistant _createDefaultAssistant() {
    return AiAssistant(
      id: 'multimodal-assistant',
      name: 'Multimodal Assistant',
      avatar: 'ğŸ­',
      systemPrompt: '',
      temperature: 0.7,
      topP: 1.0,
      maxTokens: 1000,
      contextLength: 1,
      streamOutput: false,
      isEnabled: true,
      createdAt: DateTime.now(),
      updatedAt: DateTime.now(),
      description: 'Assistant for multimodal tasks',
      customHeaders: {},
      customBody: {},
      stopSequences: [],
      frequencyPenalty: 0.0,
      presencePenalty: 0.0,
      enableCodeExecution: false,
      enableImageGeneration: false,
      enableTools: false,
      enableReasoning: false,
      enableVision: true,
      enableEmbedding: false,
    );
  }

  /// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
  void _updateStats(
      String providerId, String capability, bool success, Duration duration) {
    final key = '${providerId}_$capability';
    final currentStats = _stats[key] ?? MultimodalStats();

    _stats[key] = MultimodalStats(
      totalRequests: currentStats.totalRequests + 1,
      successfulRequests: success
          ? currentStats.successfulRequests + 1
          : currentStats.successfulRequests,
      failedRequests: success
          ? currentStats.failedRequests
          : currentStats.failedRequests + 1,
      totalDuration: currentStats.totalDuration + duration,
      lastRequestTime: DateTime.now(),
    );
  }

  /// ç”Ÿæˆè¯·æ±‚ID
  String _generateRequestId() {
    return 'multimodal_${DateTime.now().millisecondsSinceEpoch}';
  }

  /// è·å–å¤šæ¨¡æ€ç»Ÿè®¡ä¿¡æ¯
  Map<String, MultimodalStats> getMultimodalStats() => Map.from(_stats);
}

/// è¯­éŸ³è½¬æ–‡å­—å“åº”
class SpeechToTextResponse {
  final String text;
  final String? language;
  final Duration duration;
  final bool isSuccess;
  final String? error;

  const SpeechToTextResponse({
    required this.text,
    required this.language,
    required this.duration,
    required this.isSuccess,
    this.error,
  });
}

/// æ–‡å­—è½¬è¯­éŸ³å“åº”
class TextToSpeechResponse {
  final Uint8List audioData;
  final Duration duration;
  final bool isSuccess;
  final String? error;

  const TextToSpeechResponse({
    required this.audioData,
    required this.duration,
    required this.isSuccess,
    this.error,
  });
}

/// å¤šæ¨¡æ€ç»Ÿè®¡ä¿¡æ¯
class MultimodalStats {
  final int totalRequests;
  final int successfulRequests;
  final int failedRequests;
  final Duration totalDuration;
  final DateTime? lastRequestTime;

  const MultimodalStats({
    this.totalRequests = 0,
    this.successfulRequests = 0,
    this.failedRequests = 0,
    this.totalDuration = Duration.zero,
    this.lastRequestTime,
  });

  double get successRate =>
      totalRequests > 0 ? successfulRequests / totalRequests : 0.0;
  Duration get averageDuration => totalRequests > 0
      ? Duration(microseconds: totalDuration.inMicroseconds ~/ totalRequests)
      : Duration.zero;
}
